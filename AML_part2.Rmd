---
title: "Applied Machine Learning Assessment: Part 2"
output: html_document
---

```{r include=FALSE, message = FALSE, warning=FALSE}

# installing relevant packages
cran.packages<-c("caret","tidyverse","rpart","rpart.plot","party",
                 "randomForest","e1071","Rtsne","dbscan","C50","UpSetR",
                 "RColorBrewer","GGally","ggfortify","reshape2","plyr",
                 "corrplot","pROC","scatterplot3d","devtools","dendextend", 
                 "magrittr","cluster", "gplots","methods","class","datasets", 
                 "caTools","ggplot2","ggdendro","doParallel","devtools",
                 "mlbench","plot3D","ROCR","UsingR","rafalib","downloader",
                 "lattice","stepPlr","arm","kernlab","nnet","neuralnet",
                 "MASS", "NeuralNetTools","ISLR", "boot","faraway","CVST",
                 "readr", "pheatmap", "cluster", "missForest", "mice", "VIM",
                 "mice", "limma", "factoextra")

bioconductor.packages<-c("EBImage")

# CRAN packages
new.packages <- cran.packages[!(cran.packages %in% 
                                installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

# bioconductor packages
if (!requireNamespace("BiocManager", quietly = TRUE))
  install.packages("BiocManager")
BiocManager::install()
new.bio.packages<-bioconductor.packages[!(bioconductor.packages 
                                         %in% installed.packages()[,"Package"])]
if(length(new.bio.packages)) BiocManager::install(new.bio.packages)

# packages from other repositories
devtools::install_github("SheffieldML/vargplvm/vargplvmR")
devtools::install_github("ManchesterBioinference/DEtime")

# set working directory
setwd("")

# load relevant packages
library(caret)
library(ggplot2); theme_set(theme_minimal())
library(UpSetR)
library(GGally)
library(rpart.plot)
library(Rtsne)
library(dplyr)
library(randomForest)
library(pheatmap)
library(cluster)
library(limma)
library(reshape2)
library(tracerer)
library(VIM)
library(magrittr)
library(mice)
library(psych)
library(missForest)
library(doParallel)
library(gridExtra)
library(grid)
library(factoextra)
library(doParallel)

```

```{r datasetB code, include=FALSE, message = FALSE, warning=FALSE}

# load data
cuomoB = read.csv('Cuomo_B_Relabelled.csv',row.names = 1) 
sum(is.na(cuomoB$classification)) # number of NAs

cuomoClass = cuomoB$classification
cuomoB$ID = rownames(cuomoB)
table(as.factor(cuomoB$classification)) # summary of types

which(duplicated(cuomoB[,-502])) # number of duplicated entries

cuomoB2 = cuomoB[-which(duplicated(cuomoB[,-502])),]

# addressing duplicated labels
a <- which(duplicated(cuomoB2[, -c(501,502)], fromLast = TRUE))

b <- which(duplicated(cuomoB2[, -c(501,502)], fromLast = FALSE))

duplicated <- cuomoB2[c(a, b),]

cuomoClass=as.data.frame(cuomoClass)
cuomoClass$ID = rownames(cuomoClass)

duplicated2 <- left_join(duplicated, cuomoClass, on="ID", all=TRUE)

duplicated2 <- duplicated2 %>%
  select(-classification, -cuomoClass)

cuomoB2 = cuomoB2[-b,]

for(i in a){
  cuomoB2$classification[i]=NA }

sum(is.na(cuomoB2$classification))

rownames(cuomoB2) = cuomoB2$ID
cuomoB2 = cuomoB2[,-502]

table(as.factor(cuomoB2$classification))

indexna = which(is.na(cuomoB2$classification))

# static imputation

cuomoB2$classification = as.factor(cuomoB2$classification)

cuomoB_mode = cuomoB2 %>%
  mutate(classification = if_else(is.na(classification),
         calc_mode(classification),
         classification))

# model-based imputation

imp <- mice(cuomoB2, method = "polyreg")
cuomoB_imp <- complete(imp)

# summary
table(cuomoB2$classification)
table(cuomoB_mode$classification)
table(cuomoB_imp$classification)

```

# Part 2

For the second part of the assessment, this analysis uses Dataset B to explore some machine learning approaches. The dataset comprises 408 cells (presented on the rows), for 500 genes (presented on the columns), with a priori defined labels for the cells. The first ML approach explored is clustering, wherein two distinct clustering approaches are applied on all features in the dataset and are compared for differences. The second ML approach explored is an ensemble of support vector machines (SVMs), which is discussed further below.

# Clustering [10 marks]

## Clustering task: [a] apply two clustering approaches on the dataset, using all features [6 marks] 

The two clustering approaches selected are hierarchical clustering and k-means clustering. The data has been split into three clusters for both approaches. To visualize the clustering, dendogram plots and k-means dimensionality plots have been generated.

```{r clustering, message = FALSE, warning = FALSE}
# clustering approach 1: hierarchical clustering

hc = hclust(dist(cuomoB_imp[,-501],method="euclidean"), method = "ward.D")
hc$labels = rownames(cuomoB_imp)
clusters.hc=as.data.frame(cutree(hc, 3))
colnames(clusters.hc)[1]="hc"
clusters.hc$ID=rownames(clusters.hc)

# draw dendogram with red borders around the 3 clusters
plot(hc, labels=FALSE, main = "Hierarchical clustering")
rect.hclust(hc, k=3, border="red")

# clustering approach 2: k means
kmeans <- kmeans(cuomoB_imp[,-501], 3, nstart = 50)
fviz_cluster(kmeans, data = cuomoB_imp[,-501],
             palette = c("#2E9FDF", "#00AFBB", "#E7B800"), 
             geom = "point",
             ellipse.type = "convex", 
             ggtheme = theme_bw(), main="K-means clustering")

clusters.km = as.data.frame(kmeans$cluster)
colnames(clusters.km)[1]="km"
clusters.km$ID=rownames(clusters.km)

```

We see in the above plots that both the dendogram and the k-means PCA plot have been separated into the three different classes. 

## Clustering task: [b] summarise the differences [4 marks]

To summarise the differences between the clustering approaches, a contingency table has been created to illustrate the number of cells assigned simultaneously to each cell type by method.

```{r summary of diffs, message = FALSE, warning = FALSE}

# summary of difference

clusters=merge(clusters.km,clusters.hc,by="ID")
with(clusters, table(km, hc))

```

For the hierarchical clustering, a large proportion of the data (164/408, i.e. ~40%) was assigned to the same cluster, i.e., type 1. For k-means clustering as well, ~43% of the data was assigned to type 1. 25.4% of the data is assigned to type 2 for hierarchical clustering and 30.1% of the data is assigned type 2 for k-means clustering. Finally, for cell type 4, 34.8% of the data is assigned to the cluster for hierarchical clustering, compared to 26.7% of the data for k-means clustering.

Both methods perform relatively similarly in assigning clusters with some discrepancy for cell types 2 and 4.

# Ensemble/Stack of Support Vector Machines (SVMs) [15 marks]

## SVMs: [a] apply (and optimise the parameters) for a SVM model on the selected dataset [7 marks]

Support vector machines (SVMs) are models of supervised learning; they find a hyperplane in an N-dimensional space(where N is the number of features) that distinctly classifies the data points. Firstly, for dataset B, an SVM model is applied and parameters are optimised after following the appropriate pre-processing steps and cross-validation procedures. Two hyperparameters (i.e., kernel and cost) were optimised, and the best SVM model was chosen, i.e, radial kernel, with c parameter 1. 

We have applied several pre-processing steps for the classification method, and therefore remove near-zero variance variables. We also remove highly correlated variables by setting a cutoff of 0.5, which means all genes and features that have a correlation larger than 0.5 are excluded from the analyses. We divide our data into a training and test set (with a 63-37 split). 

```{r svm, message=FALSE, warning=FALSE}

cuomoData = cuomoB_imp[,-501]
cuomoClass = cuomoB_imp[,501]

# preprocessing

nzv <- nearZeroVar(cuomoData, saveMetrics=T) # identify zero and near zero variance variables
cuomoData = cuomoData[,rownames(nzv[nzv$zeroVar==FALSE,])] # remove constant genes 
near.zero.variance = rownames(nzv[nzv$nzv==TRUE,]) # store nzv
print(length(near.zero.variance))

corMat <- cor(cuomoData) # identify highly correlated variables
highCorr <- findCorrelation(corMat, cutoff=0.5) 
highly.correlated = names(cuomoData)[highCorr] # store
print(length(highly.correlated)) 

# remove highly correlated and near zero var features
features.to.exclude = unique(c(highly.correlated,near.zero.variance))
print(length(features.to.exclude))
cuomoData = cuomoData[,!(colnames(cuomoData)%in%features.to.exclude)]

# parallelisation
registerDoParallel(8)
getDoParWorkers()

# split into training and test
set.seed(42)
trainIndex <- createDataPartition(y=cuomoClass, times=1, p=0.63, list=F)
classTrain <- as.factor(cuomoClass[trainIndex])
dataTrain <- cuomoData[trainIndex,]
classTest <- as.factor(cuomoClass[-trainIndex])
dataTest <- cuomoData[-trainIndex,]

# setting up pf cross-validation seeds
set.seed(42)
seeds <- vector(mode = "list", length = 11)
for(i in 1:10) seeds[[i]] <- sample.int(1000, 75)
seeds[[11]] <- sample.int(1000,1)

train_ctrl <- trainControl(method="cv",
                           number = 10,
                           preProcOptions=list(cutoff=0.75),
                           seeds = seeds)

# Linear Kernel
L_models <- train(dataTrain, classTrain,
                  method="svmLinear",
                  preProcess = c("BoxCox"),
                  tuneLength=5,
                  trControl=train_ctrl)

# Polynomial Kernel
P_models <- train(dataTrain, classTrain,
                  method="svmPoly",
                  preProcess = c("BoxCox"),
                  tuneLength=5,
                  trControl=train_ctrl)

# Radial Kernel
R_models <- train(dataTrain, classTrain,
                  method="svmRadial",
                  preProcess = c("BoxCox"),
                  tuneLength=5,
                  trControl=train_ctrl)


# compare performance across folds
resamps <- resamples(list(Linear = L_models, Poly = P_models, Radial = R_models))
summary(resamps)

resamps.df = as.data.frame(resamps)
resamps.df.melt = reshape2::melt(resamps.df,id_vars=c('Resample'))
ggplot(resamps.df.melt,aes(x=variable,y=value,color=variable))+geom_boxplot() + ggtitle("Accurary boxplot")
ggplot(resamps.df.melt,aes(x=value,y=..density..,color=variable))+geom_density()+ggtitle("Accuracy density plot")


# pick the best 

if (median(P_models$resample$Accuracy)>max(median(L_models$resample$Accuracy),median(R_models$resample$Accuracy)) |
    IQR(P_models$resample$Accuracy)<min(IQR(L_models$resample$Accuracy),IQR(R_models$resample$Accuracy))){
  bestSVM = P_models
  grid = expand.grid(.C=bestSVM$bestTune$C,.degree=bestSVM$bestTune$degree,.scale=bestSVM$bestTune$scale)
} else if (median(R_models$resample$Accuracy)>median(L_models$resample$Accuracy) |
           IQR(R_models$resample$Accuracy)<IQR(L_models$resample$Accuracy)){
  bestSVM = R_models
  grid = expand.grid(.C=bestSVM$bestTune$C,.sigma = bestSVM$bestTune$sigma)
} else {
  bestSVM = L_models
  grid = expand.grid(.C=bestSVM$bestTune$C)
}

train_ctrl <- trainControl(method="none",
                           seeds = seeds)
# fit the svm model

svmFit <- train(dataTrain, classTrain,
                method=bestSVM$method,
                preProcess = c("BoxCox"),
                tuneGrid = grid,
                trControl = train_ctrl)

svmTest = predict(svmFit,dataTest)
confusionMatrix(svmTest,classTest)

remove(corMat,cuomoB,L_models,P_models,R_models,nzv,resamps,resamps.df,resamps.df.melt,a,b,features.to.exclude,highCorr,highly.correlated,near.zero.variance)

```

All the models are achieving an accuracy of over 92%, which is quite good. The median value for the linear SVM is much lower than the polynomial and radial SVMs. The overall distribution for the linear SVM is also lower than the other two; therefore, we can exclude the linear kernel. We have used an equation to predict the best model based on median accuracy and interquartile range. The confusion matrix has yielded decent predictions, and sensitivity and specificity values are relatively high. This will be discussed further in the open-ended discussion.

## SVMs: [b] subsample the original dataset (10 iterations) – without replacement, to 63% of the data; generate the SVM models on the same parameters as before [5 marks]

We partitioned the data, i.e., we use 63% of the data and sub-sample it 10 times in this section. We train the SVM models on each of the 10 sub-samples using the same parameters as before. We then transposed the class predictions generated by the SVM model on each of subsample into a data frame. 

```{r subsample dataset, message = FALSE, warning = FALSE}

trainIndex1 <- as.data.frame(createDataPartition(y=classTrain, times=10, p=0.9, list=F))
x = as.numeric(408 - length(classTrain))
box=as.data.frame(matrix(data=NA,nrow=x,ncol=1))[,-1]
box$ID=1:x
for (i in 1:10) {
  classTrain1 <- as.factor(classTrain[trainIndex1[,i]])
  dataTrain1 <- dataTrain[trainIndex1[,i],]
  svmFit <- train(dataTrain1, classTrain1,
                  method=bestSVM$method,
                  preProcess = c("BoxCox"),
                  tuneGrid = grid,
                  trControl = train_ctrl)
  
  svmTest = predict(svmFit,dataTest)
  mat=as.data.frame(matrix(data=NA,nrow=x,ncol=1))[,-1]
  mat$test=svmTest
  mat$ID=1:x
  colnames(mat)[1]=paste("svm",i,sep="-")
  
  box=merge(box,mat,by="ID")
}
```

## SVMs: [c] on the test dataset, apply the voting approach; compare it to the outputs resulted in [a] [3 marks]

To apply the voting approach, using the dataframe created earlier with the predictions of classes per observation, we assume that the final classification is the one chosen a majority of the sub-samples. We compare the output using confusion matrix and accuracy metrics. This will be discussed further in the open-ended discussion.

```{r voting approach, message = FALSE, warning = FALSE}

box=box[,-1]
for (i in 1:149) {
  box$gr1[i]=length(which(box[i,c(1:10)]==1))
  box$gr2[i]=length(which(box[i,c(1:10)]==2))
  box$gr4[i]=length(which(box[i,c(1:10)]==4))
  box$max[i]=max(box$gr1[i],box$gr2[i],box$gr4[i])
}

box=box[,-c(1:10)]

box$gr1=as.numeric(box$gr1)
box$gr2=as.numeric(box$gr2)
box$gr4=as.numeric(box$gr4)

box = box %>%
  mutate(class = case_when(gr1 == box$max ~ 1,
                           gr2 == box$max ~ 2,
                           gr4 == box$max ~ 4))%>%
mutate(label=ifelse(box$max < 10, "Variant", "Invariant"))

ensembleSVM <- as.factor(box$class)

confusionMatrix(svmTest, classTest) # first SVM vs all SVM sub-samples
confusionMatrix(ensembleSVM,svmTest) # ensemble vs first SVM
confusionMatrix(ensembleSVM,classTest) # ensemble vs all SVM sub-samples

length(which(box$label=="Variant"))
length(which(box$label=="Invariant"))

box$actual=classTest
box$test=ifelse(box$class==box$actual,1,0)
table(box$label,box$class)
table(box$label,box$test)



```

